# -*- coding: utf-8 -*-
"""chahita_nlp_project_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NprNxuPViM0i5EyUq6oowBVUb4cPUfuo
"""

!pip install PyPDF2 nltk
import PyPDF2
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import pandas as pd

!pip install contractions
import contractions
import re
import string

!pip install PyMuPDF

from google.colab import drive
drive.mount("/content/drive")

"""### Remove non-textual data"""

import fitz

pdf_file = '/content/drive/Shared drives/NLP Project Group30/World War II Encyclopedia and Document Collection.pdf'
pdf_doc = fitz.open(pdf_file)

for page_num in range(pdf_doc.page_count):
    page = pdf_doc[page_num]
    image_blocks = page.get_images()

    # Loop through the image blocks and delete them
    for image_block in image_blocks:
        page.delete_image(image_block[0])

pdf_doc.save('/content/drive/Shared drives/NLP Project Group30/updated_book.pdf')

# read book
pdf_file = open('/content/drive/Shared drives/NLP Project Group30/updated_book.pdf', 'rb')
pdf_reader = PyPDF2.PdfReader(pdf_file)

# book to text
text = ''
total_pages = len(pdf_reader.pages)
for page_num in range(total_pages):
    page = pdf_reader.pages[page_num]
    page_text = page.extract_text()
    text += page_text

with open('/content/drive/Shared drives/NLP Project Group30/text_file1.txt', 'w') as file:
    file.write(text)

with open('/content/drive/Shared drives/NLP Project Group30/text_file1.txt', 'r') as file:
    text = file.read()

"""### Expand contractions """

#contractions
text = contractions.fix(text)

"""### Unecessary characters removal"""

# unecessary characters removal

unwanted_chars_pattern = re.compile(r'[^a-zA-Z0-9\s.,]+')
filtered_sentence = unwanted_chars_pattern.sub('', text)
filtered_sentence=filtered_sentence.strip()

filtered_sentence= ' '.join(filtered_sentence.split())

"""### Coref resolution"""

mkdir temp

cd temp

!git clone https://github.com/huggingface/neuralcoref.git
!pip install -U spacy
!python -m spacy download en

cd neuralcoref

!pip install -r requirements.txt
!pip install -e .

import neuralcoref
import spacy

!python -m spacy download en

nlp = spacy.load('en')
neuralcoref.add_to_pipe(nlp)

# Split the text into sentences
sentences = nltk.sent_tokenize(filtered_sentence)

group_size = 3
groups = [sentences[i:i+group_size] for i in range(0, len(sentences), group_size)]

modified_sentences = []
for group in groups:
    sentence_str = " ".join(group)
    doc = nlp(sentence_str)
    sent = ""
    for token in doc:
        new_val = token.text
        for cluster in doc._.coref_clusters:
            for value in cluster.mentions:
                if token.text == value.text:
                    new_val = cluster.main.text
        sent = sent + new_val + " "
    modified_sentences.append(sent)

result_text = "\n".join(modified_sentences)

with open('/content/drive/Shared drives/NLP Project Group30/text_file2.txt', 'w') as file:
    file.write(result_text)

with open('/content/drive/Shared drives/NLP Project Group30/text_file2.txt', 'r') as file:
    result_text = file.read()

"""### Stop words removal"""

words = word_tokenize(result_text)
stop_words = set(stopwords.words('english'))
custom_stop_words = stop_words - set(['do', 'not'])
filtered_words = [word for word in words if word.lower() not in custom_stop_words]
filtered_text = " ".join(filtered_words)

"""### Lemmatization"""

nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.corpus import wordnet

def get_pos_tag(tokens):
    return [nltk.tag.pos_tag([word])[0] for word in tokens]
    
def pos_tag_recognizer(tag):
    if tag.startswith("N"):
        return wordnet.NOUN
    elif tag.startswith("V"):
        return wordnet.VERB
    elif tag.startswith("R"):
        return wordnet.ADV
    elif tag.startswith("J"):
        return wordnet.ADJ
    else:
        return wordnet.NOUN

from nltk.stem import WordNetLemmatizer

def lemmatize_word(word_list):
    word_pos_tagged = get_pos_tag(word_list)
    return " ".join([WordNetLemmatizer().lemmatize(word, pos_tag_recognizer(tag)) for (word, tag) in word_pos_tagged])

filtered_text = lemmatize_word(nltk.word_tokenize(filtered_text))

with open('/content/drive/Shared drives/NLP Project Group30/text_file3.txt', 'w') as file:
    file.write(filtered_text)

with open('/content/drive/Shared drives/NLP Project Group30/text_file3.txt', 'r') as file:
    filtered_text = file.read()

"""### Sentence Classification"""

sentences = []
sentences = filtered_text.split(". ")
sentences[-1] = sentences[-1].rstrip(".")
year_pattern = r"\b\d{4}\b"
results = []

for sentence in sentences:
    year = re.search(year_pattern, sentence)
    if year:
        year = int(year.group())
        result = {"date": year, "sentence": sentence}
        results.append(result)

df = pd.DataFrame(results)

import spacy

nlp = spacy.load("en_core_web_sm")
data_country = {}
countries = []

for i, row in df.iterrows():
    year = row['date']
    sentence = row['sentence']
    doc = nlp(sentence)

    for ent in doc.ents:
        if ent.label_ == "GPE" and ent.text in open("/content/drive/Shared drives/NLP Project Group30/countries.txt").read():
            countries = ent.text
            if year not in data_country:
                data_country[year] = {}
            
            if countries not in data_country[year]:
                data_country[year][countries] = []
            
            data_country[year][countries].append(sentence)

data_person = {}
person = []

for i, row in df.iterrows():
    year = row['date']
    sentence = row['sentence']
    doc = nlp(sentence)

    for ent in doc.ents:
          if ent.label_ == "PERSON":
            persons=ent.text
            if year not in data_person:
                data_person[year] = {}
            
            if persons not in data_person[year]:
                data_person[year][persons] = []
            
            data_person[year][persons].append(sentence)

years = []
persons = []
sentences = []

for year, data in data_person.items():
    for person, sentences_list in data.items():
        for sentence in sentences_list:
            years.append(year)
            persons.append(person)
            sentences.append(sentence)

df_person = pd.DataFrame({'year': years, 'person': persons, 'sentence': sentences})

df_person = df_person.drop_duplicates()

df_person.to_csv("/content/drive/Shareddrives/NLP Project Group30/person.csv")

years = []
countries = []
sentences = []

for year, data in data_country.items():
    for country, sentences_list in data.items():
        for sentence in sentences_list:
            years.append(year)
            countries.append(country)
            sentences.append(sentence)

df_country = pd.DataFrame({'year': years, 'country': countries, 'sentence': sentences})

df_country = df_country.drop_duplicates()

df_country.to_csv("/content/drive/Shareddrives/NLP Project Group30/country.csv")

list_person = list(set(list(df_person['person'])))
list_country = list(set(list(df_country['country'])))
len(list_person), len(list_country)

pattern = r"\b(" + "|".join(list_person + list_country) + r")\b"

df['lvl1'] = " "
for i in range(0, len(df)):
  if re.search(pattern, df.loc[i,'sentence']):
    df.loc[i,'lvl1']="relevant"
  else:
    df.loc[i,'lvl1']="irrelevant"

df['lvl1'].value_counts()

df.to_csv('/content/drive/Shareddrives/NLP Project Group30/classification.csv', index=False)

df = pd.read_csv('/content/drive/Shareddrives/NLP Project Group30/classification.csv')

df.describe()

relevant_df = df[df['lvl1'] == 'relevant']
relevant_df = relevant_df.drop('lvl1', axis=1)

relevant_df['date'] = relevant_df['date'].astype(str)
relevant_df = relevant_df[relevant_df['date'].str.startswith('19')]

relevant_df = relevant_df[pd.to_datetime(relevant_df['date'], format='%Y').dt.year >= 1930]

relevant_df = relevant_df[pd.to_datetime(relevant_df['date'], format='%Y').dt.year <= 1950]

data_person = {}
person = []

for i, row in relevant_df.iterrows():
    year = row['date']
    sentence = row['sentence']
    doc = nlp(sentence)

    for ent in doc.ents:
          if ent.label_ == "PERSON":
            persons=ent.text
            if year not in data_person:
                data_person[year] = {}
            
            if persons not in data_person[year]:
                data_person[year][persons] = []
            
            data_person[year][persons].append(sentence)

years = []
persons = []
sentences = []

for year, data in data_person.items():
    for person, sentences_list in data.items():
        for sentence in sentences_list:
            years.append(year)
            persons.append(person)
            sentences.append(sentence)

df_person = pd.DataFrame({'year': years, 'person': persons, 'sentence': sentences})

df_person = df_person.drop_duplicates()

person_count = df_person.groupby('person').size().reset_index(name='count')
relevant_df_filtered = df_person.merge(person_count[person_count['count'] >= 50]['person'], on='person')

from nltk.corpus.reader.ycoe import read_alignedsent_block

grouped = relevant_df_filtered.groupby(['year', 'person'])['sentence'].apply(lambda x: ' '.join(x))
grouped = grouped.reset_index()

grouped = grouped.drop('person', axis=1)
grouped.to_csv('/content/drive/Shareddrives/NLP Project Group30/grouped.csv', index=False)

grouped.describe()

"""### Summarization"""

!pip install openai

!pip install summa

import openai
import spacy
from collections import Counter
from summa import keywords

def summarize_paragraph(paragraph):
    openai.api_key = "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"

    # using TextRank algorithm
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(paragraph)
    word_frequencies = Counter([token.text for token in doc if not token.is_stop and not token.is_punct])
    most_common_words = word_frequencies.most_common(5)
    keywords = [word[0] for word in most_common_words]

    model_engine = "text-davinci-003"
    prompt = f"{', '.join(keywords)}. {paragraph}"
    response = openai.Completion.create(engine=model_engine, prompt=prompt, temperature=0, max_tokens=100)
    summary = response.choices[0].text.strip()
    title = f"{' '.join(keywords)}: {summary}"

    if len(title.split()) > 15:
        title = " ".join(title.split()[:10]) + "..."
    return summary, title, paragraph

results = []
df_person=pd.read_csv('/content/drive/Shareddrives/NLP Project Group30/grouped.csv')
for index, row in df_person.iterrows():
    paragraph = row["sentence"]
    year = row["year"]
    summary,title,paragraph = summarize_paragraph(paragraph)
    results.append([year, paragraph, summary ,title])

df_results = pd.DataFrame(results, columns=["Year", "Paragraph","Summary" ,"Title"])

df_results.to_csv('/content/drive/Shareddrives/NLP Project Group30/new_df_results.csv', index=False)

"""### Timeline creation"""

import altair as alt
alt.data_transformers.enable('default', max_rows=None)

data = pd.read_csv("/content/drive/Shared drives/NLP Project Group30/new_df_results.csv", header=None, names=["Year","Paragraph" ,"Summary","Title"])

color_scheme = alt.Scale(
    range=['#1A237E', '#283593', '#303F9F', '#3949AB'],
    type='ordinal'
)

chart = alt.Chart(data).mark_square(size=60, stroke=None).encode(
    x=alt.X('yearmonth(Year):O', axis=alt.Axis(title='Year', domain=False)),
    y=alt.Y('Title:N', sort='-x', axis=None),
    tooltip=['Title', 'Summary']
).properties(
    width=1090
)

text = chart.mark_text(
    align='left',
    baseline='middle',
    dx=8,
    color='black',
    fontSize=12
).encode(
    text=alt.Text('Title:N'),
    x=alt.X('yearmonth(Year):O', axis=alt.Axis(title='', tickCount=30, format='%Y')),
    y=alt.Y('Title:N', sort='-x'),
)

text.configure_mark(
    width=chart.width,
    height=chart.height
)

line = chart.mark_rule(
    color='gray',
    strokeWidth=0.5
).encode(
    y=alt.Y('Title:N', sort='-x'),
)

timeline = chart + text + line

timeline.configure_view(
    fill='rgba(0, 0, 0, 0)',
    stroke=None,
    strokeWidth=0,
).configure_title(
    font='Helvetica',
    fontSize=24,
    fontWeight='bold',
    fontStyle='italic',
    anchor='start',
    color='black'
).configure_axis(
    labelFont='Arial',
    labelFontSize=14,
    labelFontWeight='normal',
    labelFontStyle='italic',
    titleFont='Verdana',
    titleFontSize=18,
    titleFontWeight='bold',
    titleFontStyle='normal',
    tickSize=5,
    tickColor='gray',
    labelColor='black'
).configure_legend(
    titleFont='Helvetica',
    titleFontSize=16,
    titleFontWeight='bold',
    titleFontStyle='normal',
    labelFont='Arial',
    labelFontSize=14,
    labelColor='black'
).configure_legend(
    symbolStrokeWidth=3,
    symbolSize=300
)

timeline
